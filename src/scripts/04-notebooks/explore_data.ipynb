{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JoHtaF4MVXq"
   },
   "outputs": [],
   "source": [
    "project = \"\" # @param {type:\"string\"}\n",
    "location = \"\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VI0xLxmm7HBa"
   },
   "source": [
    "# Example 1: Query a table with SQL and magic commands\n",
    "\n",
    "Colab Notebooks in BigQuery supports execution of queries and generates results in a dataframe.\n",
    "Below code queries the BQ table created by Spark in previous step, and provides the results in a Pandas Dataframe `revenue_report_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "sql",
    "id": "Y1CHG5H6D9Yp",
    "tags": [
     "table-ref:mrobles-dev-sandbox.ecomm_demo.revenue_report"
    ]
   },
   "outputs": [],
   "source": [
    "# sql_engine: bigquery\n",
    "# output_variable: revenue_report_df\n",
    "# start _sql\n",
    "_sql = \"\"\"\n",
    "SELECT * FROM `datalake_demo.revenue_report`\n",
    "\"\"\" # end _sql\n",
    "from google.colab.sql import bigquery as _bqsqlcell\n",
    "revenue_report_df = _bqsqlcell.run(_sql)\n",
    "revenue_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB9Wacg6NSmU"
   },
   "outputs": [],
   "source": [
    "# This is a Code cell, it has context so the Dataframe created by previous SQL\n",
    "# cell is available here too.\n",
    "revenue_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "sql",
    "id": "sGr5PSXgN86m",
    "tags": [
     "table-ref:mrobles-dev-sandbox.ecomm_demo.customers"
    ]
   },
   "outputs": [],
   "source": [
    "# sql_engine: bigquery\n",
    "# output_variable: customers_df\n",
    "# start _sql\n",
    "_sql = \"\"\"\n",
    "-- BigLake External tables created for the migrated data are also accesible in BigQuery.\n",
    "SELECT * FROM `datalake_demo.customers` LIMIT 10;\n",
    "\"\"\" # end _sql\n",
    "from google.colab.sql import bigquery as _bqsqlcell\n",
    "customers_df = _bqsqlcell.run(_sql)\n",
    "customers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYt5QGSf67N3"
   },
   "source": [
    "# Example 2: Query a table with BigQuery DataFrames\n",
    "\n",
    "With BigQuery DataFrames, you can use many familiar Pandas methods, but the processing happens in BigQuery rather than the runtime, allowing you to work with larger DataFrames that would otherwise not fit in the runtime memory.\n",
    "Learn more here: https://cloud.google.com/python/docs/reference/bigframes/latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUsQzlgjiA_G"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This cell uses BigFrames to query the table generated by Spark previously.\n",
    "import bigframes.pandas as bf\n",
    "\n",
    "df = bf.read_gbq(f\"{project}.datalake_demo.revenue_report\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCNdWPAg6qSP"
   },
   "source": [
    "# Example 3: Query a table with remote Spark session\n",
    "\n",
    "The code below shows how both BigLake External tables and BigQuery tables can be consumed from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEFREPzx6lsb"
   },
   "outputs": [],
   "source": [
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "\n",
    "## Create a Spark session with default configuration:\n",
    "spark = DataprocSparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AyZuQMpZE92"
   },
   "outputs": [],
   "source": [
    "# The code below shows how BigLake External tables can be consumed from PySpark\n",
    "\n",
    "# Load data from BigQuery.\n",
    "table = spark.read.format('bigquery') \\\n",
    "  .option('table', f\"{project}.datalake_demo.customers\") \\\n",
    "  .load()\n",
    "table.createOrReplaceTempView('customersTempView')\n",
    "\n",
    "# Explore the data and schema.\n",
    "table_data = spark.sql(\n",
    "    'SELECT * FROM customersTempView')\n",
    "table_data.show()\n",
    "table_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXboyy4sPQBm"
   },
   "outputs": [],
   "source": [
    "# The code below shows how both BigQuery tables can be consumed from PySpark\n",
    "\n",
    "# Load data from BigQuery.\n",
    "table = spark.read.format('bigquery') \\\n",
    "  .option('table', f\"{project}.datalake_demo.revenue_report\") \\\n",
    "  .load()\n",
    "table.createOrReplaceTempView('revenueReportTempView')\n",
    "\n",
    "# Explore the data and schema.\n",
    "table_data = spark.sql(\n",
    "    'SELECT * FROM revenueReportTempView')\n",
    "table_data.show()\n",
    "table_data.printSchema()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "cell_execution_strategy": "setup",
   "name": "explore-data-demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
